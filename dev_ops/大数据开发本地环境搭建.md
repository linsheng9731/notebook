# 大数据开发本地环境搭建
## HDFS
```
export HADOOP_CONF_DIR=/Users/damon/Documents/WorkSpace/Open-Source/bigdata/hadoop-2.7.3/etc/hadoop/
```
确保 ssh localhost 能通
集群模式确保防火墙已经关闭

```
./bin/hadoop namenode -format
./sbin/start-dfs.sh
./sbin/start-yarn.sh
```
core-site.xml

```
<configuration>
	 <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
</property>
 <property>
    <name>hadoop.tmp.dir</name>
    <value>/Users/damon/Documents/WorkSpace/Open-Source/bigdata/hadoop-2.7.3/tmp</value>
  </property>
 
</configuration>
```
Hdfs status: http://localhost:9870/
Yarn job status: localhost:8088/cluster/apps/SUBMITTED 

跑个例子：
```
bin/hdfs dfs -mkdir input
bin/hdfs dfs -put etc/hadoop/*.xml input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-alpha2.jar grep input output 'dfs[a-z.]+'
```
关闭safemode
```
./bin/hadoop dfsadmin -safemode leave 
```
## Hive
https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-InstallationandConfiguration

```
export HIVE_HOME=/Users/damon/Documents/WorkSpace/Open-Source/bigdata/apache-hive-2.3.3-bin/

$HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
$HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse

/bin/schematool -dbType derby -initSchema

./bin/hive，进入hive窗口
show databases 查看所有的database;
use origin_ennenergy_onecard; 则使用origin_ennenergy_onecard数据库
show create table M_BD_T_GAS_ORDER_INFO_H 则可以查看table在hdfs上的存储路径
```
## Hbase
https://www.cnblogs.com/sheeva/p/4815893.html 单机

hbase-site.xml:

```
<configuration>
    <property>
        <name> hbase.rootdir </name>
        <value>hdfs://localhost:9000/hbase</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>localhost</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
</configuration>
```
```
./bin/start-hbase.sh
./bin/stop-hbase.sh
./bin/hbase shell
$ !tables
```

## Zookeeper
```
cd conf
cp zoo_sample.cfg zoo.cfg
#zk localhost:2181
./bin/zkServer.sh start 
./bin/zkCli.sh -server localhost:2181
```

## Phoenix
https://segmentfault.com/a/1190000002936080

确保zookeeper能连接
```
cp phoenix-4.8.0-HBase-1.1-server.jar  /hbase-1.1.5/lib/
```
重启hbase
```
stop-hbase.sh
start-hbase.sh
```
启动phoenix
```
./bin/sqlline.py
```

## Kafka
确保本地已经启动zk

```
# 启动
./bin/kafka-server-start.sh config/server.properties

# 创建一个topic
./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topic-name

# 查看所有topic
./bin/kafka-topics.sh --list --zookeeper localhost:2181

# 启动一个生产者
./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topic-name

# 启动一个消费者
./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic topic-name --from-beginning
```


